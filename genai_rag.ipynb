{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import streamlit as st\n",
    "# openai_api = st.secrets[\"OPENAI_API_KEY\"]\n",
    "openai_api = \"OpenAI_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex,Settings\n",
    "from llama_index.core.node_parser import SentenceSplitter,SemanticSplitterNodeParser\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.llms.openai import OpenAI as OpenAIsum\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.storage import StorageContext\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.storage.chat_store import SimpleChatStore\n",
    "from llama_index.core.memory import ChatMemoryBuffer,ChatSummaryMemoryBuffer\n",
    "\n",
    "import chromadb\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "chat_store = SimpleChatStore()\n",
    "chat_memory = ChatMemoryBuffer.from_defaults(\n",
    "    token_limit=3000,\n",
    "    chat_store=chat_store,\n",
    "    chat_store_key=\"user1\",\n",
    ")\n",
    "\n",
    "\n",
    "sum_llm = OpenAIsum(api_key=openai_api, model=\"gpt-3.5-turbo\", max_tokens=256)\n",
    "chat_summary_memory = ChatSummaryMemoryBuffer.from_defaults(\n",
    "    token_limit=256,\n",
    "    chat_store=chat_store,\n",
    "    chat_store_key=\"user1\",\n",
    "    llm = sum_llm,\n",
    "    tokenizer_fn = tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n",
    ")\n",
    "\n",
    "\n",
    "chat_store = SimpleChatStore.from_persist_path(\n",
    "    persist_path=\"chat_store.json\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "db = chromadb.PersistentClient(path=\"./vec_db\")\n",
    "\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\",api_key=openai_api,)\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context,)\n",
    "vector_query_engine = vector_index.as_chat_engine(chat_memory=chat_summary_memory,storage_context=storage_context,use_async=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "doc_store_path = \"./doc_dir\"\n",
    "\n",
    "def filter_unsaved(file_paths:list):\n",
    "    for i in file_paths:\n",
    "        if os.path.isfile(os.path.join(doc_store_path,os.path.basename(i))):\n",
    "            file_paths.remove(i)\n",
    "            print(\"File already exist : {}\".format(i))\n",
    "        else:\n",
    "            shutil.copy2(i,doc_store_path)\n",
    "    return file_paths\n",
    "\n",
    "def add_doc(file_paths:list):\n",
    "    print(file_paths)\n",
    "    file_paths = filter_unsaved(file_paths)\n",
    "    print(file_paths)\n",
    "    if len(file_paths) == 0:\n",
    "        return\n",
    "    docs = SimpleDirectoryReader(input_files=file_paths).load_data()\n",
    "    splitter = SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=95, embed_model=Settings.embed_model,chunk_size=256)\n",
    "    nodes = splitter.get_nodes_from_documents(docs)\n",
    "    vector_index2 = VectorStoreIndex(nodes)\n",
    "    vector_index.insert_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./llamaindex_doc.pdf']\n",
      "['./llamaindex_doc.pdf']\n"
     ]
    }
   ],
   "source": [
    "add_doc([\n",
    "    './llamaindex_doc.pdf'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_engine = vector_query_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaIndex is a project that serves as a toolkit designed to easily connect LLM's with external data. It provides data structures for indexing data for various LLM tasks, data connectors to common data sources, cost transparency tools, and customizable parameters for different use cases. This allows users to query the indices in a general-purpose manner to achieve tasks typically done with an LLM, such as question-answering.\n"
     ]
    }
   ],
   "source": [
    "response = q_engine.chat(\"what is llamaindex?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make a simple Llama project in Python, you can follow these steps:\n",
      "1. Set up a Flask backend for your project.\n",
      "2. Initialize your index by writing code to load documents and create a GPTVectorStoreIndex.\n",
      "3. Ensure you have the necessary dependencies like python3.11, llama_index, flask, typescript, and react.\n",
      "4. Optionally, you can use a FastAPI server instead of Flask if you prefer.\n",
      "5. Remember to handle user index queries in your Flask server.\n",
      "6. Make sure to include the necessary code to interact with LlamaIndex and set up your server to respond to user queries effectively.\n"
     ]
    }
   ],
   "source": [
    "response = q_engine.chat(\"tell me how to make simple llama project inpython?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
